{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LX5dYgOCTE2g3kcpKd1tGux1SoEPbbHX",
      "authorship_tag": "ABX9TyOUM41E+jaoVo6Cnke2Cam8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfarrukhm/making_models_small/blob/main/omitting_a_class_KD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J0qhkGWC-57V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "transform = torchvision.transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.50,0.5,0.5], std=[0.50,0.5,0.5]),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 dataset:\n",
        "\n",
        "train_dataset =datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset =  datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "# train_dataset = torch.utils.data.Subset((datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)),range(10000)) # Changed transforms to transform\n",
        "# test_dataset =  torch.utils.data.Subset(datasets.CIFAR10(root='./data', train=False, download=True, transform=transform),range(2000)) # Changed transforms to transform\n",
        "#Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwxmKx1I_GO8",
        "outputId": "9a97d916-e50a-4c96-9a3b-c5a1123bd3e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class to omit. In this case, let's say we're removing class 'cat' (index 3).\n",
        "class_to_omit = 3  # 'Cat' class\n",
        "\n",
        "# Filter out all images of class '3' from the training set\n",
        "filtered_images = []\n",
        "filtered_labels = []\n",
        "\n",
        "for img, label in train_dataset:\n",
        "    if label != class_to_omit:  # Skip images of class '3' (Cat)\n",
        "        filtered_images.append(img)\n",
        "        filtered_labels.append(label)\n",
        "\n",
        "# Now we have the filtered dataset without the 'cat' class\n",
        "# Create a new dataset object with the filtered data\n",
        "filtered_dataset = torch.utils.data.TensorDataset(torch.stack(filtered_images), torch.tensor(filtered_labels))\n",
        "\n",
        "len(filtered_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ujKBQUJnqs",
        "outputId": "7313efe6-6a90-4149-9041-2f8592e2754b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data_loader = torch.utils.data.DataLoader(filtered_dataset, batch_size=64, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "TmyfTbmCKIyt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class to omit. In this case, let's say we're removing class 'cat' (index 3).\n",
        "class_to_omit = 3  # 'Cat' class\n",
        "\n",
        "# Filter out all images of class '3' from the training set\n",
        "filtered_images = []\n",
        "filtered_labels = []\n",
        "\n",
        "for img, label in train_dataset:\n",
        "    if label == class_to_omit:  # Now keep images of class '3' (Cat) only\n",
        "        filtered_images.append(img)\n",
        "        filtered_labels.append(label)\n",
        "\n",
        "# Now we have the filtered dataset without the 'cat' class\n",
        "# Create a new dataset object with the filtered data\n",
        "cat_only_dataset = torch.utils.data.TensorDataset(torch.stack(filtered_images), torch.tensor(filtered_labels))\n",
        "cat_only_data_loader = torch.utils.data.DataLoader(cat_only_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "len(cat_only_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb_v4Fy8RF2O",
        "outputId": "08bd208b-456c-4ce9-f81e-d748c67c96ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the teacher model\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features=nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128,64,kernel_size=3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,32,kernel_size=3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.classifier=nn.Sequential(\n",
        "            nn.Linear(2048,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x=self.features(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        x=self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "## Student Model (way lighter than the teacher model).\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features=nn.Sequential(\n",
        "            nn.Conv2d(3,10,kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.Conv2d(10,10,kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier=nn.Sequential(\n",
        "            nn.Linear(640,256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256,num_classes)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x=self.features(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        x=self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7Sotr_qB_tVj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the teacher and student model before the distiallation with cross entropy\n",
        "device=\"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "from collections import defaultdict\n",
        "log_dict=defaultdict(list)\n",
        "\n",
        "def train(model, train_loader, num_epochs, learning_rate,device, save_model_path=None):\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss=0\n",
        "        for images, labels in train_loader:\n",
        "            images,labels=images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs=model(images)\n",
        "            loss=loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss+=loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "    if save_model_path is not None:\n",
        "        torch.save(model.state_dict(),save_model_path)\n",
        "\n",
        "def test(model, test_loader, device,bias_adjustment_value=None,omitted_class_index=None):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct_predictions=0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images,labels=images.to(device), labels.to(device)\n",
        "            outputs=model(images)\n",
        "            if bias_adjustment_value is not None and omitted_class_index is not None:\n",
        "                outputs[:,omitted_class_index]+=bias_adjustment_value\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total+=labels.size(0)\n",
        "            correct_predictions+=(labels==predicted).sum()\n",
        "\n",
        "    accuracy=100*correct_predictions/total\n",
        "    print(f\"Total correct predictions: {correct_predictions}\")\n",
        "    print(f\"Total labels: {total}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    return correct_predictions, total, accuracy\n"
      ],
      "metadata": {
        "id": "7BjxTeVL__oZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGVoRQaIQ9M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the trained teacher model\n",
        "teacher_model=TeacherModel()\n",
        "teacher_state_dict=torch.load(\"/content/drive/MyDrive/deep_generative_models/knowledge_distillation/cifar_teacher_v1.pt\",map_location=device)\n",
        "teacher_model.load_state_dict(teacher_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC-FVNsOLeMk",
        "outputId": "593c0b5f-418a-4068-e450-4f9c62962389"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-cbec3ca06097>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher_state_dict=torch.load(\"/content/drive/MyDrive/deep_generative_models/knowledge_distillation/cifar_teacher_v1.pt\",map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knowledge_distillation(teacher,student, train_loader, num_epochs,\n",
        "                                 learning_rate, temperature, soft_training_loss_weight,\n",
        "                                 ce_loss_weight, device):\n",
        "    teacher.eval()\n",
        "    teacher.to(device)\n",
        "    student.train()\n",
        "    student.to(device)\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    loss_fn=torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss=0\n",
        "        for images, labels in train_loader:\n",
        "            images,labels=images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                teacher_logits=teacher(images)\n",
        "\n",
        "            student_logits=student(images)\n",
        "\n",
        "            soft_targets=torch.softmax(teacher_logits/temperature,\n",
        "                                                dim=-1)\n",
        "            soft_probs=torch.softmax(student_logits/temperature,dim=-1)\n",
        "\n",
        "            # porbability distribution loss\n",
        "            kl_div_loss= torch.sum(soft_targets*(soft_targets.log()-soft_probs.log()))/soft_probs.size(0)*temperature**2  #Kullback-Leibler (KL) divergence between two probabilit distributions modeling the same random variable\n",
        "\n",
        "            # classification loss which is cross-entropy loss\n",
        "            ce_loss=loss_fn(student_logits, labels)\n",
        "\n",
        "            # weighted sum of the two losses\n",
        "            loss=soft_training_loss_weight*kl_div_loss + ce_loss_weight*ce_loss\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss+=loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "\n",
        "# starting teaching the student\n",
        "student_model=StudentModel()\n",
        "torch.manual_seed(2342)\n",
        "train_knowledge_distillation(teacher=teacher_model, student=student_model, train_loader=filtered_data_loader, num_epochs=10, learning_rate=0.001, temperature=7,\n",
        "                             soft_training_loss_weight=0.1, ce_loss_weight=0.9, device=device)\n",
        "\n",
        "\n",
        "torch.save(student_model.state_dict(),\"/content/drive/MyDrive/deep_generative_models/knowledge_distillation/cifar_student_trained_with_teacher_minus1class.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAQY5HcOLsaO",
        "outputId": "e29f42e0-6102-43cd-df13-1219d03e09be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 4.536415325308388\n",
            "Epoch 2/10, Loss: 3.5634797256101263\n",
            "Epoch 3/10, Loss: 3.1084851910444824\n",
            "Epoch 4/10, Loss: 2.790835639800538\n",
            "Epoch 5/10, Loss: 2.5543414633721113\n",
            "Epoch 6/10, Loss: 2.3723912095143036\n",
            "Epoch 7/10, Loss: 2.209739116443829\n",
            "Epoch 8/10, Loss: 2.0731710718775336\n",
            "Epoch 9/10, Loss: 1.9602221245454117\n",
            "Epoch 10/10, Loss: 1.8630866493013771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the trained teacher model\n",
        "student_model=StudentModel()\n",
        "student_state_dict=torch.load(\"/content/drive/MyDrive/deep_generative_models/knowledge_distillation/cifar_student_trained_with_teacher_minus1class.pt\",map_location=device)\n",
        "student_model.load_state_dict(student_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x_efkq-abpZ",
        "outputId": "c173d7e8-b5b9-44b7-982d-70cd6ec361ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-575618db976a>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  student_state_dict=torch.load(\"/content/drive/MyDrive/deep_generative_models/knowledge_distillation/cifar_student_trained_with_teacher_minus1class.pt\",map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test student model on full test data set\n",
        "test_student = test(student_model, test_loader, device)  # with no bias adjustemnt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEqZDxDpQxqU",
        "outputId": "aadebe0e-a554-4265-fe01-3c90410ef9f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total correct predictions: 6415\n",
            "Total labels: 10000\n",
            "Test Accuracy: 64.1500015258789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test student model on full test data set\n",
        "test_student = test(student_model, test_loader, device,bias_adjustment_value=3.5,omitted_class_index=3)  # with bias adjustment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o00oIjW3cue-",
        "outputId": "d564730a-b4b0-45c3-de8c-7ea29f1f538a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total correct predictions: 6539\n",
            "Total labels: 10000\n",
            "Test Accuracy: 65.38999938964844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the student model on cat only data set\n",
        "test_student = test(student_model, cat_only_data_loader, device,bias_adjustment_value=3.5,omitted_class_index=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiZOOHxuRYht",
        "outputId": "6e924168-c90a-4831-b72c-426b6644b090"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total correct predictions: 1621\n",
            "Total labels: 5000\n",
            "Test Accuracy: 32.41999816894531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bias adjustment in the code\n",
        "import torch\n",
        "\n",
        "# Example logits from the student model (e.g., after a forward pass)\n",
        "logits = torch.randn(10, 10)  # 10 samples, 10 classes (CIFAR-10 classes)\n",
        "\n",
        "# Bias adjustment value\n",
        "bias_adjustment_value = 3.5  # Example bias value to be added to the \"cat\" class (index 3)\n",
        "\n",
        "# Index of the \"cat\" class in the output logits\n",
        "cat_class_index = 3  # In CIFAR-10, let's assume \"cat\" is class 3\n",
        "\n",
        "# Apply the bias adjustment: Add bias to the logits of the \"cat\" class\n",
        "logits[:, cat_class_index] += bias_adjustment_value\n",
        "\n",
        "# Now you can apply softmax to get probabilities and make predictions\n",
        "probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "predictions = torch.argmax(probs, dim=1)\n"
      ],
      "metadata": {
        "id": "VqYr5dh4TNaU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits[:,3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8QgceXcFR9",
        "outputId": "3c123ca5-6277-42f6-b542-69a8f4efbfbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.6950, 1.7896, 4.0834, 3.5368, 3.7830, 3.5069, 3.3803, 2.7273, 2.1615,\n",
              "        4.6468])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}